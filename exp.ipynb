{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import VHR10\n",
    " \n",
    "dataset = VHR10(root=\"...\", download=True, checksum=True)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    " \n",
    "for batch in dataloader:\n",
    "    image = batch[\"image\"]\n",
    "    label = batch[\"label\"]\n",
    " \n",
    "    # train a model, or make predictions using a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 @20120905@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20120905@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20131005@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20131005@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20170319@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20170319@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20171027@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20171027@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20191107@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20191107@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20211104@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20211104@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20220203@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20220203@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 @20250117@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif 已重命名为 @20250117@120.40747060326400@36.60902770136900@120.46053192297900@36.58563292439400@.tif\n",
      "文件 .ipynb_checkpoints 已重命名为 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 定义文件夹路径\n",
    "folder_path = \"/root/workspace/ctf53sc7v38s73e0mksg/maps/new_qd_years\"\n",
    "\n",
    "# 定义原始字符串和目标字符串\n",
    "original_suffix = \"@116.35551452636700@40.09815882135800@116.44632339477501@40.15118932709900@.tif\"\n",
    "new_suffix = \"@116.35551452636700@40.15118932709900@116.44632339477501@40.09815882135800@.tif\"\n",
    "\n",
    "# 遍历文件夹中的所有文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 检查文件名是否以原始字符串结尾\n",
    "    # if filename.endswith(original_suffix):\n",
    "        # 构造新的文件名\n",
    "    new_filename = filename.replace(original_suffix, new_suffix)\n",
    "        \n",
    "    # 构造完整的文件路径\n",
    "    old_file_path = os.path.join(folder_path, filename)\n",
    "    new_file_path = os.path.join(folder_path, new_filename)\n",
    "    \n",
    "    # 重命名文件\n",
    "    os.rename(old_file_path, new_file_path)\n",
    "    print(f\"文件 {filename} 已重命名为 {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# assume 'x' is your input tensor with shape (32, 1, 2560)\n",
    "x = torch.randn(32, 1, 2560)\n",
    "\n",
    "# perform PCA on the input tensor\n",
    "components, explained_variance = torch.pca_lowrank(x, q=1280)\n",
    "\n",
    "# reconstruct the reduced tensor\n",
    "x_reduced = torch.matmul(x, components)\n",
    "\n",
    "print(x_reduced.shape)  # should be (32, 1, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# assume 'x' is your input tensor with shape (32, 1, 2560)\n",
    "x = torch.randn(32, 1, 2560)\n",
    "\n",
    "# perform SVD (PCA) on the input tensor\n",
    "u, s, v = torch.svd(x)\n",
    "\n",
    "# select the top 1280 singular values and corresponding singular vectors\n",
    "u_reduced = u[:, :, :1280]\n",
    "s_reduced = s[:, :1280]\n",
    "v_reduced = v[:, :1280, :]\n",
    "\n",
    "# reconstruct the reduced tensor\n",
    "x_reduced = torch.matmul(u_reduced, torch.diag_embed(s_reduced))\n",
    "print(x_reduced.shape)  # should be (32, 1, 1280)\n",
    "x_reduced = torch.matmul(x_reduced, v_reduced)\n",
    "\n",
    "print(x_reduced.shape)  # should be (32, 1, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pca_reduction(data, n_components):\n",
    "    # 标准化数据\n",
    "    data_mean = torch.mean(data, dim=0, keepdim=True)\n",
    "    data_std = torch.std(data, dim=0, keepdim=True)\n",
    "    data_normalized = (data - data_mean) / data_std\n",
    "\n",
    "    # 计算协方差矩阵\n",
    "    cov_matrix = torch.cov(data_normalized.view(-1, data.size(-1)).T)\n",
    "\n",
    "    # 计算特征值和特征向量\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # 选择前 n_components 个特征向量\n",
    "    top_eigenvectors = eigenvectors[:, -n_components:]\n",
    "\n",
    "    # 投影数据\n",
    "    reduced_data = torch.matmul(data_normalized.view(-1, data.size(-1)), top_eigenvectors)\n",
    "\n",
    "    # 调整形状为 (32, 1, 100)\n",
    "    reduced_data = reduced_data.view(data.size(0), data.size(1), n_components)\n",
    "\n",
    "    return reduced_data\n",
    "\n",
    "# 假设你有一个形状为 (32, 1, 240) 的张量\n",
    "input_tensor = torch.randn(32, 1, 240)\n",
    "\n",
    "# 进行 PCA 降维到 100 维\n",
    "output_tensor = pca_reduction(input_tensor, n_components=100)\n",
    "\n",
    "print(output_tensor.shape)  # 输出应为 torch.Size([32, 1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"train_dc.py\", \"-bb\", \"ResNet50\", \"-agg\", \"MixVPR\", \"--device\", \"cuda\", \"--num_workers\", \"16\", \"--batch_size\", \"64\", \"--dataset_name\", \"ct01\", \"--train_set_path\", \"/root/workspace/ctf53sc7v38s73e0mksg/maps/HC-100-700-seperate-height\", \"--test_set_path\", \"/root/workspace/ctf53sc7v38s73e0mksg/maps/HC-cities-test\", \"-ltc\", \"3\", \"4\", \"-ltf\", \"1\", \"-ipe\", \"2000\", \"--N\", \"2\", \"--fft\", \"--fft_log_base\", \"1.5\"\n"
     ]
    }
   ],
   "source": [
    "command = 'train_dc.py -bb ResNet50 -agg MixVPR --device cuda --num_workers 16 --batch_size 64 --dataset_name ct01 --train_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-100-700-seperate-height --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-cities-test -ltc 3 4 -ltf 1 -ipe 2000 --N 2 --fft --fft_log_base 1.5'\n",
    "\n",
    "parts = command.split()\n",
    "result = ', '.join([f'\"{part}\"' for part in parts])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"--M\", \"100\", \n",
      "\"--N\", \"2\", \n",
      "\"--min_images_per_class\", \"20\", \n",
      "\"--iterations_per_epoch\", \"2000\", \n",
      "\"--batch_size\", \"64\", \n",
      "\"--train_resize\", \"320\", \"320\", \n",
      "\"--test_resize\", \"320\", \"320\", \n",
      "\"--backbone\", \"ResNet50\", \n",
      "\"--aggregator\", \"MixVPR\", \n",
      "\"--threshold\", \"100\", \n",
      "\"--num_workers\", \"16\", \n",
      "\"--device\", \"cuda\", \n",
      "\"--exp_name\", \"ct01-6years-test\", \n",
      "\"--train_set_path\", \"/root/workspace/ctf53sc7v38s73e0mksg/maps/ct01_125_years\", \n",
      "\"--test_set_path\", \"/root/workspace/ctf53sc7v38s73e0mksg/maps/ct01-test-125_new\", \n",
      "\"--dataset_name\", \"ct01-6years-MixVPR\", \n",
      "\"--classifier_type\", \"QAMC\", \n",
      "\"--test_dataset_name\", \"ct01-ct\", \n",
      "\"-csv\", \"/root/workspace/ctf53sc7v38s73e0mksg/code/Height-Classify/logs/hc-ResNet50-MixVPR/2025-01-15_14-03-32/results.csv\", \n",
      "\"--resume_model\", \"/root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/logs/Baseline-ct01-6years-train-M100/2025-03-26_21-00-58/best_model.pth\", \n",
      "\"--model_classifier_layer\"\n"
     ]
    }
   ],
   "source": [
    "command = 'train_dc.py -bb ResNet50 -agg MixVPR --device cuda --num_workers 16 --batch_size 64 --dataset_name ct01 --train_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-100-700-seperate-height --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-cities-test -ltc 3 4 -ltf 1 -ipe 2000 --N 2 --fft --fft_log_base 1.5'\n",
    "command = 'he_train.py --M 300 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 336 448 --test_resize 336 --backbone radio_v2.5-h --train_blocks_num 2 --return_token --threshold 300 --num_workers 16 --device cuda --exp_name ct01-5years --train_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/ct01_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/ct01-test-125 --dataset_name ct01-5years'\n",
    "\n",
    "command = \"--threshold 30 --device cuda --exp_name ct01_csv --test_set_list ct01 --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-cities-test --test_resize 336\"\n",
    "\n",
    "command = '-bb ResNet50 -agg MixVPR --device cuda --num_workers 16 --batch_size 64 --dataset_name 2022 2013 --train_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-100-700-seperate-height --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/qd -ltc 3 4 -ltf 1 -ipe 2000 --N 2'\n",
    "\n",
    "command = 'train_dc.py -bb ResNet50 -agg MixVPR --device cuda --num_workers 16 --batch_size 64 --dataset_name 2022 qd_train --train_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/HC-100-700-seperate-height --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/qd -ltc 3 4 -ltf 1 -ipe 2000 --N 2'\n",
    "\n",
    "command = '--test_resize 320 320 --device cuda --exp_name ct01-6years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/ct01_125_years/202306_125 --dataset_name ct01-6years -csv /root/workspace/ctf53sc7v38s73e0mksg/code/Height-Classify/logs/hc-ResNet50-MixVPR/2025-01-15_14-03-32/results.csv --test_dataset_name HC-ct01'\n",
    "\n",
    "command = '--M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 336 448 --test_resize 336 --backbone radio_v2.5-h --train_blocks_num 2 --return_token --threshold 100 --num_workers 16 --device cuda --exp_name qd-6years-trace1 --train_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years --test_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years/2022_125 --dataset_name qd-6years --resume_model /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/logs/qd-6years-train-M100/2025-01-15_22-40-46/best_model.pth -csv /root/workspace/ctf53sc7v38s73e0mksg/code/Height-Classify/logs/hc-ResNet50-MixVPR/2025-01-17_11-16-20/results.csv --test_dataset_name HC-qd1'\n",
    "\n",
    "command = 'he_train.py --M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 336 448 --test_resize 336 --backbone radio_v2.5-h --train_blocks_num 2 --aggregator MixVPR --threshold 200 --num_workers 16 --device cuda --exp_name qd-8years-train-M100 --train_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/tmp_img/qd_test_cut --dataset_name qd-8years --classifier_type QAMC --test_dataset_name trace1_ct'\n",
    "\n",
    "command = 'he_train.py --M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 336 448 --test_resize 336 --backbone EfficientNet_B5 --aggregator MixVPR --threshold 100 --num_workers 16 --device cuda --exp_name qd-8years-train-M100 --train_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/tmp_img/qd_test_cut --dataset_name qd-8years --classifier_type QAMC --test_dataset_name trace1_ct'\n",
    "\n",
    "command = 'save_descriptor.py --M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 336 448 --test_resize 336 --backbone EfficientNet_B5 --aggregator MixVPR --threshold 100 --num_workers 16 --device cuda --exp_name qd-8years-train-M100 --train_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/tmp_img/qd_test_cut --dataset_name qd-8years --classifier_type QAMC --test_dataset_name trace1_ct'\n",
    "\n",
    "command = \"python he_output_cut.py --test_resize 224 224 --device cuda --exp_name ct01-6years --dataset_name ct01-6years --num_workers 8 -csv /root/workspace/ctf53sc7v38s73e0mksg/code/Height-Classify/logs/hc-ResNet50-MixVPR/2025-01-15_14-03-32/results.csv --test_dataset_name HC-ct01 --model AnyLoc\"\n",
    "\n",
    "command = 'he_train.py --M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 224 224 --test_resize 224 224 --backbone dinov2_vitb14 --aggregator CricaVPR --threshold 100 --num_workers 16 --device cuda --exp_name Baseline-qd-8years-train-M100 --train_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/tmp_img/qd_test_cut --dataset_name qd-8years --classifier_type QAMC --test_dataset_name trace_ct'\n",
    "\n",
    "command = \"--M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 224 224 --test_resize 224 224 --backbone dinov2_vitb14 -ds finetune --aggregator salad --threshold 100 --num_workers 16 --device cuda --exp_name Baseline-qd-8years-train-M100 --train_set_path /root/workspace/ctvsuas7v38s73eo9qlg/maps/qd_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/tmp_img/qd_test_cut --dataset_name qd-8years --classifier_type QAMC --test_dataset_name trace_ct\"\n",
    "\n",
    "command = '--M 100 --N 2 --min_images_per_class 20 --iterations_per_epoch 2000 --batch_size 64 --train_resize 320 320 --test_resize 320 320 --backbone ResNet50 --aggregator MixVPR --threshold 100 --num_workers 16 --device cuda --exp_name ct01-6years-test --train_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/ct01_125_years --test_set_path /root/workspace/ctf53sc7v38s73e0mksg/maps/ct01-test-125_new --dataset_name ct01-6years-MixVPR --classifier_type QAMC --test_dataset_name ct01-ct -csv /root/workspace/ctf53sc7v38s73e0mksg/code/Height-Classify/logs/hc-ResNet50-MixVPR/2025-01-15_14-03-32/results.csv --resume_model /root/workspace/ctf53sc7v38s73e0mksg/code/UAV-Divide-Classify/logs/Baseline-ct01-6years-train-M100/2025-03-26_21-00-58/best_model.pth --model_classifier_layer'\n",
    "\n",
    "parts = command.split()\n",
    "formatted_parts = []\n",
    "for part in parts:\n",
    "    if part.startswith(\"--\") or part.startswith(\"-\"):\n",
    "        formatted_parts.append(\"\\n\" + f'\"{part}\"')\n",
    "    else:\n",
    "        formatted_parts.append(f'\"{part}\"')\n",
    "\n",
    "result = \", \".join(formatted_parts)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
